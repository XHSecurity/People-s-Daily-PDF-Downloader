"""
äººæ°‘æ—¥æŠ¥PDFä¸‹è½½å·¥å…·ï¼ˆPyMuPDFä¼˜åŒ–ç‰ˆï¼‰
æ”¹è¿›ç‚¹ï¼š
1. ä½¿ç”¨PyMuPDFæ›¿ä»£PyPDF2è§£å†³åˆå¹¶è­¦å‘Š
2. ä¿æŒå®Œæ•´ä»£ç†åŠŸèƒ½
3. å¢å¼ºPDFåˆå¹¶ç¨³å®šæ€§
"""
import os
import re
import sys
import time
import shutil
import argparse
import logging
import datetime
from typing import Tuple, Optional, List
from urllib.parse import urlparse

import requests
import fitz  # PyMuPDF
from rich.progress import (
    Progress,
    BarColumn,
    DownloadColumn,
    TextColumn,
    TimeRemainingColumn,
    TransferSpeedColumn,
    TaskID,
)
from rich.console import Console
from rich.logging import RichHandler

# åŸºç¡€ç›®å½•é…ç½®
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
os.makedirs(os.path.join(BASE_DIR, "logs"), exist_ok=True)

# å…¨å±€é…ç½®
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}
TIMEOUT = 15
MAX_RETRIES = 3

# åˆå§‹åŒ–æ§åˆ¶å°å’Œæ—¥å¿—
console = Console()
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        RichHandler(console=console, rich_tracebacks=True, show_path=False),
        logging.FileHandler(
            os.path.join(BASE_DIR, "logs/rmrb.log"),
            encoding="utf-8"
        ),
    ],
)
logger = logging.getLogger("RMZ_Downloader")


def init_environment() -> Tuple[str, str]:
    """åˆå§‹åŒ–ç¯å¢ƒå¹¶æ¸…ç†ä¸´æ—¶ç›®å½•"""
    download_dir = os.path.join(BASE_DIR, "download")
    temp_dir = os.path.join(BASE_DIR, "temp")

    for dir_path in [download_dir, temp_dir]:
        os.makedirs(dir_path, exist_ok=True)

    # æ¸…ç†ä¸´æ—¶ç›®å½•
    for f in os.listdir(temp_dir):
        file_path = os.path.join(temp_dir, f)
        try:
            if os.path.isfile(file_path):
                os.unlink(file_path)
        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {file_path} - {str(e)}")

    return temp_dir, download_dir


def validate_date(input_date: Optional[str]) -> str:
    """éªŒè¯å•ä¸ªæ—¥æœŸ"""
    today = datetime.date.today()

    if not input_date:
        return today.strftime("%Y-%m/%d")

    formats = [
        "%Y%m%d",  # YYYYMMDD
        "%Y-%m-%d",  # ISOæ ¼å¼
        "%Y/%m/%d",  # æ–œæ æ ¼å¼
        "%Y-%m/%d",  # ç›®æ ‡æ ¼å¼
        "%Y%m/%d",  # æ··åˆæ ¼å¼
    ]

    for fmt in formats:
        try:
            dt = datetime.datetime.strptime(input_date, fmt).date()
            if dt > today:
                raise ValueError("æ—¥æœŸä¸èƒ½è¶…è¿‡ä»Šå¤©")
            if dt.year < 2003:
                raise ValueError("ä»…æ”¯æŒ2003å¹´åŠä¹‹åçš„æŠ¥çº¸")
            return dt.strftime("%Y-%m/%d")
        except ValueError:
            continue

    logger.error(f"æ— æ•ˆæ—¥æœŸæ ¼å¼: {input_date} (æ”¯æŒæ ¼å¼: YYYYMMDD/YYYY-MM-DD/YYYY-MM/ddç­‰)")
    sys.exit(1)


def validate_date_range(range_str: str) -> List[datetime.date]:
    """éªŒè¯å¹¶è§£ææ—¥æœŸèŒƒå›´"""
    try:
        clean_str = re.sub(r"[^0-9]", "", range_str)
        if len(clean_str) != 16:
            raise ValueError("æ— æ•ˆçš„æ—¥æœŸèŒƒå›´æ ¼å¼")

        start_str = clean_str[:8]
        end_str = clean_str[8:]
        start_date = datetime.datetime.strptime(start_str, "%Y%m%d").date()
        end_date = datetime.datetime.strptime(end_str, "%Y%m%d").date()

        if start_date > end_date:
            raise ValueError("å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸ")
        if end_date > datetime.date.today():
            raise ValueError("ç»“æŸæ—¥æœŸä¸èƒ½è¶…è¿‡ä»Šå¤©")
        if start_date.year < 2003:
            raise ValueError("ä»…æ”¯æŒ2003å¹´åŠä¹‹åçš„æŠ¥çº¸")

        date_list = []
        current_date = start_date
        while current_date <= end_date:
            date_list.append(current_date)
            current_date += datetime.timedelta(days=1)

        return date_list
    except Exception as e:
        logger.error(f"æ—¥æœŸèŒƒå›´è§£æå¤±è´¥: {str(e)}")
        sys.exit(1)


def parse_date(target_date: str) -> Tuple[str, str, str]:
    """è§£ææ—¥æœŸä¸ºä¸åŒæ ¼å¼"""
    dt = datetime.datetime.strptime(target_date, "%Y-%m/%d")
    return (
        target_date,
        dt.strftime("%Y%m%d"),
        dt.strftime("%Y%m/%d")
    )


def safe_request(url: str, proxies: Optional[dict]) -> Optional[requests.Response]:
    """å¸¦é‡è¯•æœºåˆ¶çš„è¯·æ±‚å‡½æ•°"""
    for _ in range(MAX_RETRIES):
        try:
            response = requests.get(
                url,
                headers=HEADERS,
                timeout=TIMEOUT,
                verify=False,
                allow_redirects=False,
                proxies=proxies
            )
            if response.status_code == 200:
                return response
            if response.status_code == 404:
                return None
        except Exception as e:
            logger.warning(f"è¯·æ±‚å¤±è´¥: {url} - {str(e)}")
            time.sleep(1)
    return None


def get_page_info(old_url: str, new_url: str, proxies: Optional[dict]) -> Tuple[int, bool]:
    """è·å–é¡µæ•°å¹¶åˆ¤æ–­ç‰ˆæœ¬"""
    if response := safe_request(new_url, proxies):
        if (pages := len(re.findall(r'pageLink', response.text))) > 0:
            return pages, True

    if response := safe_request(old_url, proxies):
        return len(re.findall(r'nbs', response.text)), False

    logger.error("æ— æ³•è·å–æŠ¥çº¸ä¿¡æ¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æ—¥æœŸ")
    sys.exit(1)


def download_pdf(url: str, filename: str, temp_dir: str,
                 progress: Progress, task_id: TaskID,
                 proxies: Optional[dict]) -> bool:
    """å¸¦è¿›åº¦æ¡å’Œé‡è¯•çš„ä¸‹è½½å‡½æ•°"""
    for retry in range(MAX_RETRIES):
        try:
            response = requests.get(url, headers=HEADERS, stream=True,
                                    timeout=30, proxies=proxies)
            response.raise_for_status()

            if 'application/pdf' not in response.headers.get('Content-Type', ''):
                logger.warning(f"éPDFå†…å®¹: {url}")
                return False

            total_size = int(response.headers.get('content-length', 0))
            file_path = os.path.join(temp_dir, filename)

            progress.update(task_id, total=total_size, visible=True)

            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=2 * 1024 * 1024):  # å¢å¤§å—å¤§å°
                    if chunk:
                        f.write(chunk)
                        progress.update(task_id, advance=len(chunk))

            if os.path.getsize(file_path) >= 1024:
                return True
        except Exception as e:
            logger.warning(f"ä¸‹è½½å¤±è´¥ ({retry + 1}/{MAX_RETRIES}): {filename} - {str(e)}")
            time.sleep(1)
    return False


def download_edition(target_date: str, temp_dir: str, output_dir: str,
                     progress: Optional[Progress] = None,
                     main_task: Optional[TaskID] = None,
                     proxies: Optional[dict] = None) -> bool | None:
    """ä¸»ä¸‹è½½æµç¨‹ï¼ˆæ”¯æŒå¤–éƒ¨è¿›åº¦æ¡ï¼‰"""
    old_fmt, file_fmt, new_fmt = parse_date(target_date)
    output_path = os.path.join(output_dir, f"People's.Daily.{file_fmt}.pdf")

    if os.path.exists(output_path):
        logger.info(f"æ–‡ä»¶å·²å­˜åœ¨: {os.path.basename(output_path)}")
        return True

    old_cover = f"http://paper.people.com.cn/rmrb/html/{old_fmt}/nbs.D110000renmrb_01.htm"
    new_cover = f"http://paper.people.com.cn/rmrb/pc/layout/{new_fmt}/node_01.html"

    total_pages, is_new = get_page_info(old_cover, new_cover, proxies)
    logger.info(f"æ£€æµ‹åˆ°{target_date}å…±{total_pages}é¡µ ({'æ–°ç‰ˆ' if is_new else 'æ—§ç‰ˆ'})")

    local_progress = progress or Progress(
        TextColumn("[bold blue]{task.fields[filename]}", justify="right"),
        BarColumn(bar_width=None),
        "[progress.percentage]{task.percentage:>3.1f}%",
        "â€¢",
        DownloadColumn(),
        "â€¢",
        TransferSpeedColumn(),
        "â€¢",
        TimeRemainingColumn(),
        console=console,
    )
    use_local_progress = not progress

    success = 0
    task_id = TaskID(-1)

    try:
        if use_local_progress:
            local_progress.start()

        task_id = local_progress.add_task(
            "downloading",
            filename=f"{target_date} åˆå§‹åŒ–...",
            total=0,
            visible=True
        )

        if main_task and progress:
            progress.update(
                main_task,
                description=f"[cyan]ä¸‹è½½ {target_date}",
                refresh=True
            )

        for page in range(1, total_pages + 1):
            if main_task and progress and progress.tasks[main_task].finished:
                raise KeyboardInterrupt()

            filename = f"rmrb{file_fmt}{page:02d}.pdf"
            local_progress.update(
                task_id,
                filename=f"{target_date} ç¬¬{page:02d}é¡µ",
                refresh=True
            )

            if is_new:
                node_url = f"http://paper.people.com.cn/rmrb/pc/layout/{new_fmt}/node_{page:02d}.html"
                if not (response := safe_request(node_url, proxies)):
                    continue
                if pdf_matches := re.findall(r'(/attachement.*?\.pdf)', response.text):
                    url = f"http://paper.people.com.cn/rmrb/pc/{pdf_matches[0]}"
                else:
                    logger.error(f"ç¬¬{page}é¡µé“¾æ¥æœªæ‰¾åˆ°")
                    continue
            else:
                url = f"http://paper.people.com.cn/rmrb/images/{old_fmt}/rmrb{file_fmt}{page:02d}.pdf"

            if download_pdf(url, filename, temp_dir, local_progress, task_id, proxies):
                success += 1

        if success < total_pages:
            logger.warning(f"æˆåŠŸä¸‹è½½{success}/{total_pages}é¡µ")
            return False
        return True

    except KeyboardInterrupt:
        logger.warning("ç”¨æˆ·ç»ˆæ­¢ä¸‹è½½")
        return False
    except Exception as e:
        logger.error(f"ä¸‹è½½å¼‚å¸¸: {str(e)}")
        return False
    finally:
        local_progress.remove_task(task_id)
        if use_local_progress:
            local_progress.stop()


def merge_pdfs(temp_dir: str, output_dir: str) -> bool:
    """ä½¿ç”¨PyMuPDFåˆå¹¶PDFæ–‡ä»¶"""
    try:
        # è·å–æ’åºåçš„PDFæ–‡ä»¶åˆ—è¡¨
        pdf_files = sorted(
            [f for f in os.listdir(temp_dir) if f.endswith(".pdf")],
            key=lambda x: int(x[-6:-4])  # ä»æ–‡ä»¶åæå–é¡µç 
        )

        if not pdf_files:
            logger.error("æ²¡æœ‰æ‰¾åˆ°å¯åˆå¹¶çš„æ–‡ä»¶")
            return False

        # åˆ›å»ºæ–°æ–‡æ¡£
        doc = fitz.open()

        for filename in pdf_files:
            file_path = os.path.join(temp_dir, filename)

            # è·³è¿‡ç©ºæ–‡ä»¶
            if os.path.getsize(file_path) < 1024:
                logger.warning(f"è·³è¿‡æ— æ•ˆæ–‡ä»¶: {filename}")
                continue

            try:
                src = fitz.open(file_path)
                doc.insert_pdf(src)  # æ’å…¥æ•´ä¸ªæ–‡æ¡£
                src.close()
            except Exception as e:
                logger.error(f"æ–‡ä»¶åˆå¹¶å¤±è´¥: {filename} - {str(e)}")
                return False

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        output_name = f"People's.Daily.{pdf_files[0][4:12]}.pdf"
        output_path = os.path.join(output_dir, output_name)

        # ä¿å­˜åˆå¹¶åçš„æ–‡æ¡£
        doc.save(output_path, deflate=True, garbage=3)
        doc.close()

        logger.info(f"åˆå¹¶æˆåŠŸ: {output_path}")
        return True
    except Exception as e:
        logger.critical(f"åˆå¹¶å¤±è´¥: {str(e)}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description="äººæ°‘æ—¥æŠ¥PDFä¸‹è½½å·¥å…·",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        "-d", "--date",
        type=str,
        help="æŒ‡å®šæ—¥æœŸ (æ ¼å¼: YYYYMMDD/YYYY-MM-DD/YYYY-MM/dd)"
    )
    parser.add_argument(
        "-r", "--range",
        type=str,
        help="æ—¥æœŸèŒƒå›´ (æ ¼å¼: YYYYMMDD-YYYYMMDD æˆ– YYYY-MM-DD-YYYY-MM-DD)"
    )
    parser.add_argument(
        "-p","--proxy",
        type=str,
        help="è®¾ç½®ä»£ç†æœåŠ¡å™¨ï¼ˆæ ¼å¼ï¼šåè®®://åœ°å€:ç«¯å£ å¦‚ socks5://127.0.0.1:1080ï¼‰"
    )
    args = parser.parse_args()

    if args.date and args.range:
        logger.error("ä¸èƒ½åŒæ—¶ä½¿ç”¨ -d å’Œ -r å‚æ•°")
        sys.exit(1)
    if not args.date and not args.range:
        logger.error("å¿…é¡»æŒ‡å®šæ—¥æœŸå‚æ•° (-d) æˆ–æ—¥æœŸèŒƒå›´å‚æ•° (-r)")
        sys.exit(1)

    # ä»£ç†é…ç½®å¤„ç†
    proxies = None
    if args.proxy:
        try:
            parsed = urlparse(args.proxy)
            if not parsed.scheme or not parsed.hostname or not parsed.port:
                raise ValueError("ä»£ç†æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º åè®®://åœ°å€:ç«¯å£")
            if parsed.scheme.lower() not in ['http', 'https', 'socks5', 'socks5h']:
                raise ValueError(f"ä¸æ”¯æŒçš„ä»£ç†åè®®: {parsed.scheme}")

            proxies = {'http': args.proxy, 'https': args.proxy}

            console.rule("[bold]ä»£ç†é…ç½®[/bold]")
            console.print(f"â€¢ çŠ¶æ€ï¼š[bold green]å·²å¯ç”¨[/bold green]")
            console.print(f"â€¢ åè®®ï¼š[cyan]{parsed.scheme.upper()}[/cyan]")
            console.print(f"â€¢ åœ°å€ï¼š[cyan]{parsed.hostname}[/cyan]:[cyan]{parsed.port}[/cyan]")
            if parsed.username or parsed.password:
                console.print(
                    f"â€¢ è®¤è¯ï¼šç”¨æˆ·[cyan]{parsed.username or 'æ— '}[/cyan] å¯†ç [cyan]{'*' * 3 if parsed.password else 'æ— '}[/cyan]")
        except Exception as e:
            logger.error(f"ä»£ç†é…ç½®é”™è¯¯: {str(e)}")
            sys.exit(1)
    else:
        console.rule("[bold]ä»£ç†é…ç½®[/bold]")
        console.print("â€¢ çŠ¶æ€ï¼š[bold yellow]æœªå¯ç”¨[/bold yellow]")

    temp_dir, output_dir = init_environment()
    console.rule("[bold cyan]äººæ°‘æ—¥æŠ¥PDFä¸‹è½½å·¥å…·[/bold cyan]")
    console.print("â€¢ ç¨‹åºä½œè€…ï¼š[bold magenta]XHSecurity[/bold magenta]", style="bold magenta")
    console.print(f"â€¢ ç¨‹åºç‰ˆæœ¬: [bold yellow]V1.0ï¼ˆå¢å¼ºç‰ˆï¼‰[/bold yellow]", style="bold yellow")
    console.print("â€¢ æ›´æ–°æ—¥æœŸ: [bold green]2025å¹´04æœˆ08æ—¥[/bold green]", style="bold green")
    console.print("â€¢ ç¨‹åºç”¨é€”: [bold purple]ä¸‹è½½äººæ°‘æ—¥æŠ¥PDFæŠ¥çº¸[/bold purple]", style="bold purple")
    console.print("â€¢ å…è´£å£°æ˜: [bold red]è¯¥ç¨‹åºä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ç”¨é€”ï¼Œä»»ä½•éæ³•ç”¨é€”ä¸ä½œè€…æ— å…³ã€‚[/bold red]", style="bold red")

    try:
        if args.range:
            date_list = validate_date_range(args.range)
            total_days = len(date_list)
            logger.info(f"å‡†å¤‡ä¸‹è½½ {total_days} å¤©çš„æŠ¥çº¸")

            with Progress(
                    TextColumn("[bold cyan]{task.description}"),
                    BarColumn(bar_width=None),
                    "[progress.percentage]{task.percentage:>3.0f}%",
                    "â€¢",
                    TimeRemainingColumn(),
                    console=console,
                    refresh_per_second=10
            ) as main_progress:
                main_task = main_progress.add_task(
                    "æ‰¹é‡ä¸‹è½½è¿›åº¦",
                    total=total_days,
                    visible=True
                )

                for idx, date_obj in enumerate(date_list, 1):
                    current_date = date_obj.strftime("%Y-%m-%d")
                    target_date = date_obj.strftime("%Y-%m/%d")

                    main_progress.update(
                        main_task,
                        description=f"å¤„ç† {current_date} ({idx}/{total_days})",
                        advance=1,
                        refresh=True
                    )

                    try:
                        current_temp_dir, _ = init_environment()

                        download_success = download_edition(
                            target_date,
                            current_temp_dir,
                            output_dir,
                            progress=main_progress,
                            main_task=main_task,
                            proxies=proxies
                        )

                        if download_success:
                            merge_success = merge_pdfs(current_temp_dir, output_dir)
                            if not merge_success:
                                logger.error(f"{current_date} åˆå¹¶å¤±è´¥")
                        else:
                            logger.warning(f"{current_date} ä¸‹è½½æœªå®Œæˆ")

                    except Exception as e:
                        logger.error(f"{current_date} å¤„ç†å¤±è´¥: {str(e)}")
                    finally:
                        shutil.rmtree(current_temp_dir, ignore_errors=True)

            logger.info("ğŸ‰ æ‰¹é‡ä¸‹è½½å®Œæˆï¼")

        else:
            target_date = validate_date(args.date)
            logger.info(f"ç›®æ ‡æ—¥æœŸ: {target_date}")
            download_success = download_edition(target_date, temp_dir, output_dir, proxies=proxies)
            if download_success:
                merge_success = merge_pdfs(temp_dir, output_dir)
                if not merge_success:
                    logger.error("åˆå¹¶å¤±è´¥")
            shutil.rmtree(temp_dir, ignore_errors=True)
            logger.info("ğŸ‰ ä»»åŠ¡å®Œæˆï¼")

    except KeyboardInterrupt:
        logger.warning("ç”¨æˆ·ç»ˆæ­¢æ“ä½œ")
        sys.exit(130)
    except Exception as e:
        logger.critical(f"ç¨‹åºå¼‚å¸¸: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()